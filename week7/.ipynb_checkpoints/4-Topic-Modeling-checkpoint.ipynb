{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data. There are also matrix factorization techniques for topic modeling such as Latent Senmantic Indexing(LSI) and Non-Negative Matrix Factorization (NMF) \n",
    "\n",
    "\n",
    "- Input:\n",
    "    To use a topic modeling technique, you need to provide:\n",
    "    + (1) a document-term matrix;\n",
    "    + (2) the number of topics you would like the algorithm to pick up;\n",
    "    + (3) number of iterations.\n",
    "\n",
    "\n",
    "- Gensim will go through the process of finding the best word distribution for each topic and best topic distribution for each document\n",
    "\n",
    "\n",
    "- Output:\n",
    "    + (1) The top words in each topic; \n",
    "    + (2) your job as a human is to interpret the results and see if the mix of words in each topic make sense. \n",
    "    + (3) If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 7468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject   ...    zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0   ...      0    0         0     0        0   \n",
       "anthony     0        0       0   ...      0    0         0     0        0   \n",
       "bill        1        0       0   ...      0    0         0     1        1   \n",
       "bo          0        1       0   ...      0    0         0     1        0   \n",
       "dave        0        0       0   ...      0    0         0     0        0   \n",
       "hasan       0        0       0   ...      2    1         0     1        0   \n",
       "jim         0        0       0   ...      0    0         0     0        0   \n",
       "joe         0        0       0   ...      0    0         0     0        0   \n",
       "john        0        0       0   ...      0    0         0     0        0   \n",
       "louis       0        0       0   ...      0    0         0     2        0   \n",
       "mike        0        0       0   ...      0    0         2     1        0   \n",
       "ricky       0        1       1   ...      0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  Ã©clair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7468 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('./pickle/dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/osx-64::pytest-remotedata==0.3.0=py37_0\n",
      "  - defaults/osx-64::notebook==5.6.0=py37_0\n",
      "  - defaults/osx-64::jupyterlab_launcher==0.13.1=py37_0\n",
      "  - defaults/osx-64::jupyter==1.0.0=py37_7\n",
      "  - defaults/osx-64::service_identity==17.0.0=py37h28b3542_0\n",
      "  - defaults/osx-64::bcrypt==3.2.0=py37haf1e3a3_0\n",
      "  - defaults/osx-64::prometheus_client==0.3.1=py37h28b3542_0\n",
      "  - conda-forge/osx-64::conda==4.8.4=py37hc8dfbb8_2\n",
      "  - defaults/osx-64::spyder==3.3.1=py37_1\n",
      "  - conda-forge/noarch::jupyterlab==2.1.0=py_0\n",
      "  - defaults/osx-64::parsel==1.5.2=py37_0\n",
      "  - defaults/osx-64::flask-cors==3.0.6=py37_0\n",
      "  - defaults/osx-64::anaconda-project==0.8.2=py37_0\n",
      "  - defaults/osx-64::pytest-arraydiff==0.2=py37h39e3cac_0\n",
      "  - defaults/noarch::pytest-runner==5.2=py_0\n",
      "  - defaults/noarch::jupyterlab_server==1.0.0=py_0\n",
      "  - defaults/osx-64::automat==0.7.0=py37_0\n",
      "  - defaults/osx-64::twisted==19.2.0=py37h1de35cc_0\n",
      "  - defaults/osx-64::ipywidgets==7.4.1=py37_0\n",
      "  - defaults/osx-64::pytest-openfiles==0.3.0=py37_0\n",
      "  - conda-forge/noarch::voila==0.0.9=py_0\n",
      "  - defaults/osx-64::pytest==3.8.0=py37_0\n",
      "  - conda-forge/osx-64::jupyter_server==0.0.3=py37_0\n",
      "  - defaults/osx-64::_anaconda_depends==5.3.1=py37_0\n",
      "  - defaults/osx-64::anaconda-navigator==1.9.2=py37_0\n",
      "  - defaults/osx-64::anaconda-client==1.7.2=py37_0\n",
      "  - defaults/osx-64::sphinx==1.7.9=py37_0\n",
      "  - defaults/osx-64::pytest-doctestplus==0.1.3=py37_0\n",
      "  - defaults/osx-64::_ipyw_jlab_nb_ext_conf==0.1.0=py37_0\n",
      "  - defaults/osx-64::widgetsnbextension==3.4.1=py37_0\n",
      "  - defaults/osx-64::astropy==3.0.4=py37h1de35cc_0\n",
      "  - defaults/osx-64::blaze==0.11.3=py37_0\n",
      "  - defaults/osx-64::pytest-astropy==0.4.0=py37_0\n",
      "  - defaults/osx-64::numpydoc==0.8.0=py37_0\n",
      "  - defaults/osx-64::scrapy==2.3.0=py37_0\n",
      "  - defaults/osx-64::anaconda==custom=py37_1\n",
      "  - defaults/osx-64::conda-build==3.15.1=py37_0\n",
      "  - defaults/noarch::itemloaders==1.0.2=py_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/wenweipeng/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    attrs-20.2.0               |     pyh9f0ad1d_0          41 KB  conda-forge\n",
      "    boto3-1.15.18              |     pyh9f0ad1d_0          69 KB  conda-forge\n",
      "    botocore-1.18.18           |     pyh9f0ad1d_0         4.1 MB  conda-forge\n",
      "    bz2file-0.98               |             py_0           9 KB  conda-forge\n",
      "    certifi-2020.6.20          |   py37h2987424_2         151 KB  conda-forge\n",
      "    flask-1.1.1                |             py_0          69 KB  conda-forge\n",
      "    gensim-3.8.3               |   py37h570ac47_2        22.7 MB  conda-forge\n",
      "    lxml-4.3.3                 |   py37h08abf6f_0         1.3 MB  conda-forge\n",
      "    openssl-1.1.1h             |       haf1e3a3_0         1.9 MB  conda-forge\n",
      "    pip-20.2.4                 |             py_0         1.1 MB  conda-forge\n",
      "    requests-2.24.0            |     pyh9f0ad1d_0          47 KB  conda-forge\n",
      "    s3transfer-0.3.3           |             py_3          51 KB  conda-forge\n",
      "    smart_open-1.9.0           |             py_0          56 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        31.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  attrs              conda-forge/noarch::attrs-20.2.0-pyh9f0ad1d_0\n",
      "  boto3              conda-forge/noarch::boto3-1.15.18-pyh9f0ad1d_0\n",
      "  botocore           conda-forge/noarch::botocore-1.18.18-pyh9f0ad1d_0\n",
      "  bz2file            conda-forge/noarch::bz2file-0.98-py_0\n",
      "  flask              conda-forge/noarch::flask-1.1.1-py_0\n",
      "  gensim             conda-forge/osx-64::gensim-3.8.3-py37h570ac47_2\n",
      "  lxml               conda-forge/osx-64::lxml-4.3.3-py37h08abf6f_0\n",
      "  pip                conda-forge/noarch::pip-20.2.4-py_0\n",
      "  requests           conda-forge/noarch::requests-2.24.0-pyh9f0ad1d_0\n",
      "  s3transfer         conda-forge/noarch::s3transfer-0.3.3-py_3\n",
      "  smart_open         conda-forge/noarch::smart_open-1.9.0-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                          2020.6.20-py37hc8dfbb8_0 --> 2020.6.20-py37h2987424_2\n",
      "  openssl                                 1.1.1g-haf1e3a3_1 --> 1.1.1h-haf1e3a3_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()\n",
    "# each row is a document(transcript) and each columns is a term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenweipeng/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"./pickle/cv_stop.pkl\", \"rb\"))\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:42:19,794 : INFO : using symmetric alpha at 0.5\n",
      "2020-10-23 15:42:19,794 : INFO : using symmetric eta at 0.5\n",
      "2020-10-23 15:42:19,795 : INFO : using serial LDA version on this node\n",
      "2020-10-23 15:42:19,800 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-10-23 15:42:19,924 : INFO : -9.389 per-word bound, 670.3 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:19,925 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-10-23 15:42:19,946 : INFO : topic #0 (0.500): 0.008*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.004*\"going\" + 0.004*\"come\" + 0.004*\"want\" + 0.004*\"good\" + 0.004*\"theres\"\n",
      "2020-10-23 15:42:19,947 : INFO : topic #1 (0.500): 0.007*\"fucking\" + 0.005*\"shit\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"hes\" + 0.005*\"fuck\" + 0.005*\"really\" + 0.004*\"thing\" + 0.004*\"little\" + 0.004*\"cause\"\n",
      "2020-10-23 15:42:19,948 : INFO : topic diff=1.084363, rho=1.000000\n",
      "2020-10-23 15:42:20,086 : INFO : -8.064 per-word bound, 267.6 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,087 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-10-23 15:42:20,104 : INFO : topic #0 (0.500): 0.007*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"theyre\" + 0.005*\"say\" + 0.005*\"going\" + 0.004*\"theres\" + 0.004*\"want\" + 0.004*\"life\" + 0.004*\"didnt\"\n",
      "2020-10-23 15:42:20,105 : INFO : topic #1 (0.500): 0.007*\"fucking\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"hes\" + 0.005*\"shit\" + 0.005*\"little\" + 0.005*\"went\" + 0.004*\"day\" + 0.004*\"fuck\" + 0.004*\"thing\"\n",
      "2020-10-23 15:42:20,106 : INFO : topic diff=0.244748, rho=0.577350\n",
      "2020-10-23 15:42:20,229 : INFO : -7.997 per-word bound, 255.5 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,229 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-10-23 15:42:20,251 : INFO : topic #0 (0.500): 0.007*\"fucking\" + 0.007*\"shit\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"cause\" + 0.004*\"theres\"\n",
      "2020-10-23 15:42:20,252 : INFO : topic #1 (0.500): 0.007*\"fucking\" + 0.005*\"went\" + 0.005*\"say\" + 0.005*\"little\" + 0.005*\"going\" + 0.005*\"day\" + 0.005*\"hes\" + 0.004*\"fuck\" + 0.004*\"didnt\" + 0.004*\"shit\"\n",
      "2020-10-23 15:42:20,253 : INFO : topic diff=0.205258, rho=0.500000\n",
      "2020-10-23 15:42:20,373 : INFO : -7.958 per-word bound, 248.7 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,373 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-10-23 15:42:20,391 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.007*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"life\" + 0.005*\"want\" + 0.005*\"cause\" + 0.004*\"theres\"\n",
      "2020-10-23 15:42:20,392 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"little\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"fuck\" + 0.005*\"hes\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:20,393 : INFO : topic diff=0.127627, rho=0.447214\n",
      "2020-10-23 15:42:20,512 : INFO : -7.943 per-word bound, 246.1 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,513 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-10-23 15:42:20,540 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"life\" + 0.005*\"want\" + 0.004*\"theres\"\n",
      "2020-10-23 15:42:20,541 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"little\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.005*\"hes\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:20,542 : INFO : topic diff=0.081528, rho=0.408248\n",
      "2020-10-23 15:42:20,667 : INFO : -7.936 per-word bound, 244.9 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,667 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-10-23 15:42:20,686 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"life\" + 0.005*\"want\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:20,687 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"little\" + 0.005*\"didnt\" + 0.005*\"fuck\" + 0.005*\"day\" + 0.005*\"going\" + 0.004*\"hes\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:20,687 : INFO : topic diff=0.057376, rho=0.377964\n",
      "2020-10-23 15:42:20,808 : INFO : -7.932 per-word bound, 244.2 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,809 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-10-23 15:42:20,825 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:20,826 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"day\" + 0.004*\"hes\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:20,826 : INFO : topic diff=0.042858, rho=0.353553\n",
      "2020-10-23 15:42:20,944 : INFO : -7.929 per-word bound, 243.7 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:20,945 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-10-23 15:42:20,962 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:20,963 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"hes\" + 0.004*\"day\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:20,964 : INFO : topic diff=0.030795, rho=0.333333\n",
      "2020-10-23 15:42:21,085 : INFO : -7.927 per-word bound, 243.4 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:21,086 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-10-23 15:42:21,104 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:21,105 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"hes\" + 0.004*\"good\" + 0.004*\"day\"\n",
      "2020-10-23 15:42:21,106 : INFO : topic diff=0.020975, rho=0.316228\n",
      "2020-10-23 15:42:21,222 : INFO : -7.926 per-word bound, 243.3 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:21,223 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-10-23 15:42:21,239 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:21,241 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"good\" + 0.004*\"hes\" + 0.004*\"day\"\n",
      "2020-10-23 15:42:21,241 : INFO : topic diff=0.014356, rho=0.301511\n",
      "2020-10-23 15:42:21,244 : INFO : topic #0 (0.500): 0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"\n",
      "2020-10-23 15:42:21,245 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"good\" + 0.004*\"hes\" + 0.004*\"day\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"shit\" + 0.006*\"fucking\" + 0.006*\"fuck\" + 0.005*\"theyre\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"life\" + 0.004*\"hes\"'),\n",
       " (1,\n",
       "  '0.008*\"fucking\" + 0.006*\"went\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.004*\"good\" + 0.004*\"hes\" + 0.004*\"day\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:42:50,043 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-10-23 15:42:50,043 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-10-23 15:42:50,045 : INFO : using serial LDA version on this node\n",
      "2020-10-23 15:42:50,047 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-10-23 15:42:50,156 : INFO : -9.578 per-word bound, 764.4 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,157 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-10-23 15:42:50,175 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.007*\"didnt\" + 0.006*\"fucking\" + 0.006*\"hes\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"cause\" + 0.004*\"fuck\" + 0.004*\"good\"\n",
      "2020-10-23 15:42:50,176 : INFO : topic #1 (0.333): 0.007*\"fucking\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"fuck\" + 0.005*\"cause\" + 0.005*\"went\" + 0.004*\"theyre\" + 0.004*\"say\" + 0.004*\"man\" + 0.004*\"shit\"\n",
      "2020-10-23 15:42:50,178 : INFO : topic #2 (0.333): 0.008*\"fucking\" + 0.007*\"fuck\" + 0.006*\"going\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"shit\" + 0.005*\"theyre\" + 0.005*\"did\" + 0.005*\"really\" + 0.004*\"little\"\n",
      "2020-10-23 15:42:50,179 : INFO : topic diff=1.279120, rho=1.000000\n",
      "2020-10-23 15:42:50,297 : INFO : -8.185 per-word bound, 291.0 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,298 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-10-23 15:42:50,317 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.006*\"didnt\" + 0.006*\"hes\" + 0.006*\"fucking\" + 0.006*\"say\" + 0.005*\"fuck\" + 0.005*\"want\" + 0.004*\"good\" + 0.004*\"life\" + 0.004*\"thing\"\n",
      "2020-10-23 15:42:50,318 : INFO : topic #1 (0.333): 0.006*\"fucking\" + 0.006*\"day\" + 0.006*\"went\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"theyre\" + 0.004*\"goes\" + 0.004*\"really\" + 0.004*\"little\"\n",
      "2020-10-23 15:42:50,320 : INFO : topic #2 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.006*\"shit\" + 0.006*\"theyre\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"theres\" + 0.004*\"did\" + 0.004*\"really\"\n",
      "2020-10-23 15:42:50,321 : INFO : topic diff=0.404757, rho=0.577350\n",
      "2020-10-23 15:42:50,434 : INFO : -8.048 per-word bound, 264.7 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,434 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-10-23 15:42:50,452 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.006*\"hes\" + 0.006*\"didnt\" + 0.005*\"say\" + 0.005*\"fucking\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"life\"\n",
      "2020-10-23 15:42:50,453 : INFO : topic #1 (0.333): 0.007*\"fucking\" + 0.007*\"went\" + 0.006*\"day\" + 0.005*\"say\" + 0.005*\"cause\" + 0.005*\"going\" + 0.005*\"goes\" + 0.005*\"theyre\" + 0.004*\"really\" + 0.004*\"thing\"\n",
      "2020-10-23 15:42:50,454 : INFO : topic #2 (0.333): 0.012*\"fucking\" + 0.009*\"fuck\" + 0.007*\"shit\" + 0.006*\"theyre\" + 0.005*\"theres\" + 0.005*\"going\" + 0.005*\"want\" + 0.004*\"dude\" + 0.004*\"say\" + 0.004*\"man\"\n",
      "2020-10-23 15:42:50,454 : INFO : topic diff=0.340480, rho=0.500000\n",
      "2020-10-23 15:42:50,572 : INFO : -7.970 per-word bound, 250.7 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,572 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-10-23 15:42:50,590 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"hes\" + 0.005*\"didnt\" + 0.005*\"say\" + 0.005*\"want\" + 0.005*\"good\" + 0.005*\"fuck\" + 0.005*\"love\" + 0.005*\"life\" + 0.005*\"fucking\"\n",
      "2020-10-23 15:42:50,591 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"theyre\" + 0.005*\"thing\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:50,592 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.009*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.005*\"theres\" + 0.005*\"dude\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"man\" + 0.004*\"say\"\n",
      "2020-10-23 15:42:50,593 : INFO : topic diff=0.223995, rho=0.447214\n",
      "2020-10-23 15:42:50,714 : INFO : -7.938 per-word bound, 245.3 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,715 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-10-23 15:42:50,731 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"hes\" + 0.005*\"say\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"didnt\" + 0.005*\"life\" + 0.004*\"fucking\"\n",
      "2020-10-23 15:42:50,732 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:50,734 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.009*\"fuck\" + 0.009*\"shit\" + 0.007*\"theyre\" + 0.005*\"theres\" + 0.005*\"dude\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"going\" + 0.004*\"make\"\n",
      "2020-10-23 15:42:50,734 : INFO : topic diff=0.138023, rho=0.408248\n",
      "2020-10-23 15:42:50,845 : INFO : -7.927 per-word bound, 243.3 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,845 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-10-23 15:42:50,861 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"fuck\" + 0.005*\"hes\" + 0.005*\"didnt\" + 0.005*\"life\" + 0.004*\"fucking\"\n",
      "2020-10-23 15:42:50,862 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:50,863 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"fuck\" + 0.009*\"shit\" + 0.007*\"theyre\" + 0.005*\"dude\" + 0.005*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.004*\"going\" + 0.004*\"make\"\n",
      "2020-10-23 15:42:50,863 : INFO : topic diff=0.087319, rho=0.377964\n",
      "2020-10-23 15:42:50,975 : INFO : -7.922 per-word bound, 242.5 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:50,976 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-10-23 15:42:50,991 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"say\" + 0.005*\"fuck\" + 0.005*\"hes\" + 0.005*\"life\" + 0.005*\"didnt\" + 0.004*\"did\"\n",
      "2020-10-23 15:42:50,993 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:50,994 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"fuck\" + 0.010*\"shit\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.005*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"\n",
      "2020-10-23 15:42:50,994 : INFO : topic diff=0.056383, rho=0.353553\n",
      "2020-10-23 15:42:51,130 : INFO : -7.920 per-word bound, 242.1 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:51,131 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-10-23 15:42:51,148 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"life\" + 0.004*\"didnt\" + 0.004*\"did\"\n",
      "2020-10-23 15:42:51,149 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:51,150 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"fuck\" + 0.010*\"shit\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.005*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"\n",
      "2020-10-23 15:42:51,150 : INFO : topic diff=0.037057, rho=0.333333\n",
      "2020-10-23 15:42:51,286 : INFO : -7.919 per-word bound, 242.0 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:51,287 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-10-23 15:42:51,303 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"life\" + 0.004*\"didnt\" + 0.004*\"did\"\n",
      "2020-10-23 15:42:51,304 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:42:51,305 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"fuck\" + 0.010*\"shit\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.006*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"\n",
      "2020-10-23 15:42:51,306 : INFO : topic diff=0.024759, rho=0.316228\n",
      "2020-10-23 15:42:51,436 : INFO : -7.918 per-word bound, 241.9 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:42:51,437 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-10-23 15:42:51,454 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"life\" + 0.004*\"didnt\" + 0.004*\"did\"\n",
      "2020-10-23 15:42:51,455 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:51,456 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"shit\" + 0.010*\"fuck\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.006*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"\n",
      "2020-10-23 15:42:51,456 : INFO : topic diff=0.016799, rho=0.301511\n",
      "2020-10-23 15:42:51,459 : INFO : topic #0 (0.333): 0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"life\" + 0.004*\"didnt\" + 0.004*\"did\"\n",
      "2020-10-23 15:42:51,461 : INFO : topic #1 (0.333): 0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"\n",
      "2020-10-23 15:42:51,462 : INFO : topic #2 (0.333): 0.013*\"fucking\" + 0.010*\"shit\" + 0.010*\"fuck\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.006*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"shit\" + 0.005*\"good\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fuck\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"life\" + 0.004*\"didnt\" + 0.004*\"did\"'),\n",
       " (1,\n",
       "  '0.007*\"went\" + 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"thing\" + 0.005*\"theyre\" + 0.005*\"really\"'),\n",
       " (2,\n",
       "  '0.013*\"fucking\" + 0.010*\"shit\" + 0.010*\"fuck\" + 0.007*\"theyre\" + 0.006*\"dude\" + 0.006*\"theres\" + 0.005*\"man\" + 0.005*\"want\" + 0.005*\"make\" + 0.004*\"going\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:51:17,315 : INFO : using symmetric alpha at 0.25\n",
      "2020-10-23 15:51:17,316 : INFO : using symmetric eta at 0.25\n",
      "2020-10-23 15:51:17,318 : INFO : using serial LDA version on this node\n",
      "2020-10-23 15:51:17,322 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-10-23 15:51:17,448 : INFO : -9.779 per-word bound, 878.5 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:17,449 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-10-23 15:51:17,475 : INFO : topic #0 (0.250): 0.006*\"fucking\" + 0.006*\"shit\" + 0.006*\"didnt\" + 0.005*\"going\" + 0.005*\"theyre\" + 0.005*\"want\" + 0.005*\"cause\" + 0.004*\"did\" + 0.004*\"fuck\" + 0.004*\"day\"\n",
      "2020-10-23 15:51:17,476 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"thing\" + 0.005*\"good\" + 0.005*\"theyre\" + 0.005*\"day\" + 0.004*\"hes\"\n",
      "2020-10-23 15:51:17,478 : INFO : topic #2 (0.250): 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"shit\" + 0.005*\"theyre\" + 0.005*\"going\" + 0.005*\"hes\" + 0.005*\"want\" + 0.005*\"day\" + 0.004*\"life\" + 0.004*\"fuck\"\n",
      "2020-10-23 15:51:17,480 : INFO : topic #3 (0.250): 0.007*\"fucking\" + 0.006*\"fuck\" + 0.005*\"shit\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"really\" + 0.005*\"hes\" + 0.004*\"good\" + 0.004*\"thing\" + 0.004*\"cause\"\n",
      "2020-10-23 15:51:17,481 : INFO : topic diff=1.484500, rho=1.000000\n",
      "2020-10-23 15:51:17,639 : INFO : -8.340 per-word bound, 324.0 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:17,639 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-10-23 15:51:17,664 : INFO : topic #0 (0.250): 0.006*\"fucking\" + 0.006*\"didnt\" + 0.005*\"theyre\" + 0.005*\"id\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"cause\" + 0.004*\"want\" + 0.004*\"went\" + 0.004*\"really\"\n",
      "2020-10-23 15:51:17,665 : INFO : topic #1 (0.250): 0.009*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"day\" + 0.005*\"thing\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"went\" + 0.005*\"good\" + 0.005*\"theyre\"\n",
      "2020-10-23 15:51:17,666 : INFO : topic #2 (0.250): 0.006*\"say\" + 0.006*\"fucking\" + 0.005*\"going\" + 0.005*\"dad\" + 0.005*\"want\" + 0.005*\"life\" + 0.005*\"hes\" + 0.005*\"shit\" + 0.005*\"love\" + 0.004*\"did\"\n",
      "2020-10-23 15:51:17,668 : INFO : topic #3 (0.250): 0.008*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.005*\"didnt\" + 0.005*\"really\" + 0.005*\"cause\" + 0.005*\"man\" + 0.005*\"hes\" + 0.005*\"good\" + 0.005*\"say\"\n",
      "2020-10-23 15:51:17,669 : INFO : topic diff=0.595861, rho=0.577350\n",
      "2020-10-23 15:51:17,788 : INFO : -8.098 per-word bound, 274.1 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:17,788 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-10-23 15:51:17,807 : INFO : topic #0 (0.250): 0.006*\"id\" + 0.006*\"say\" + 0.006*\"fucking\" + 0.005*\"didnt\" + 0.005*\"going\" + 0.005*\"theyre\" + 0.005*\"cause\" + 0.005*\"says\" + 0.005*\"went\" + 0.004*\"goes\"\n",
      "2020-10-23 15:51:17,808 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"day\" + 0.006*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"want\" + 0.005*\"good\"\n",
      "2020-10-23 15:51:17,809 : INFO : topic #2 (0.250): 0.006*\"dad\" + 0.006*\"say\" + 0.005*\"going\" + 0.005*\"want\" + 0.005*\"love\" + 0.004*\"shes\" + 0.004*\"did\" + 0.004*\"life\" + 0.004*\"hey\" + 0.004*\"hes\"\n",
      "2020-10-23 15:51:17,810 : INFO : topic #3 (0.250): 0.009*\"shit\" + 0.008*\"fuck\" + 0.008*\"fucking\" + 0.005*\"theyre\" + 0.005*\"cause\" + 0.005*\"man\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"really\" + 0.005*\"theres\"\n",
      "2020-10-23 15:51:17,811 : INFO : topic diff=0.479793, rho=0.500000\n",
      "2020-10-23 15:51:17,931 : INFO : -7.978 per-word bound, 252.2 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:17,932 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-10-23 15:51:17,947 : INFO : topic #0 (0.250): 0.006*\"id\" + 0.006*\"say\" + 0.005*\"didnt\" + 0.005*\"fucking\" + 0.005*\"says\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"theyre\"\n",
      "2020-10-23 15:51:17,948 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"day\" + 0.006*\"want\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"good\"\n",
      "2020-10-23 15:51:17,949 : INFO : topic #2 (0.250): 0.007*\"dad\" + 0.006*\"say\" + 0.006*\"going\" + 0.005*\"want\" + 0.005*\"shes\" + 0.005*\"hey\" + 0.005*\"love\" + 0.004*\"did\" + 0.004*\"mom\" + 0.004*\"life\"\n",
      "2020-10-23 15:51:17,950 : INFO : topic #3 (0.250): 0.009*\"shit\" + 0.008*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"didnt\" + 0.005*\"life\"\n",
      "2020-10-23 15:51:17,951 : INFO : topic diff=0.305505, rho=0.447214\n",
      "2020-10-23 15:51:18,068 : INFO : -7.933 per-word bound, 244.4 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:18,068 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-10-23 15:51:18,085 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.006*\"say\" + 0.005*\"didnt\" + 0.005*\"says\" + 0.005*\"fucking\" + 0.005*\"went\" + 0.005*\"goes\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"mean\"\n",
      "2020-10-23 15:51:18,086 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"day\" + 0.006*\"want\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,087 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"say\" + 0.006*\"going\" + 0.005*\"shes\" + 0.005*\"hey\" + 0.005*\"want\" + 0.005*\"mom\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"life\"\n",
      "2020-10-23 15:51:18,087 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,088 : INFO : topic diff=0.194264, rho=0.408248\n",
      "2020-10-23 15:51:18,204 : INFO : -7.915 per-word bound, 241.3 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:18,204 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-10-23 15:51:18,220 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.006*\"say\" + 0.005*\"didnt\" + 0.005*\"says\" + 0.005*\"went\" + 0.005*\"fucking\" + 0.005*\"goes\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,221 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"day\" + 0.006*\"want\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,222 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"life\"\n",
      "2020-10-23 15:51:18,223 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,223 : INFO : topic diff=0.126478, rho=0.377964\n",
      "2020-10-23 15:51:18,349 : INFO : -7.907 per-word bound, 240.0 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:18,350 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-10-23 15:51:18,366 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.006*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"fucking\" + 0.005*\"goes\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,368 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"day\" + 0.006*\"want\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,368 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"\n",
      "2020-10-23 15:51:18,370 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,370 : INFO : topic diff=0.083571, rho=0.353553\n",
      "2020-10-23 15:51:18,487 : INFO : -7.903 per-word bound, 239.4 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:51:18,488 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-10-23 15:51:18,504 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.006*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"fucking\" + 0.005*\"goes\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,505 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"day\" + 0.006*\"want\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,506 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"\n",
      "2020-10-23 15:51:18,507 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,507 : INFO : topic diff=0.055955, rho=0.333333\n",
      "2020-10-23 15:51:18,634 : INFO : -7.902 per-word bound, 239.1 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:18,634 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-10-23 15:51:18,650 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.006*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"goes\" + 0.005*\"fucking\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,651 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"want\" + 0.006*\"day\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,652 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"\n",
      "2020-10-23 15:51:18,652 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,653 : INFO : topic diff=0.037949, rho=0.316228\n",
      "2020-10-23 15:51:18,765 : INFO : -7.901 per-word bound, 239.0 perplexity estimate based on a held-out corpus of 12 documents with 41267 words\n",
      "2020-10-23 15:51:18,766 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-10-23 15:51:18,781 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.007*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"goes\" + 0.005*\"fucking\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,781 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"want\" + 0.006*\"day\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,782 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"\n",
      "2020-10-23 15:51:18,783 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n",
      "2020-10-23 15:51:18,784 : INFO : topic diff=0.026067, rho=0.301511\n",
      "2020-10-23 15:51:18,787 : INFO : topic #0 (0.250): 0.007*\"id\" + 0.007*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"goes\" + 0.005*\"fucking\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"\n",
      "2020-10-23 15:51:18,788 : INFO : topic #1 (0.250): 0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"want\" + 0.006*\"day\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"\n",
      "2020-10-23 15:51:18,789 : INFO : topic #2 (0.250): 0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"\n",
      "2020-10-23 15:51:18,790 : INFO : topic #3 (0.250): 0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"id\" + 0.007*\"say\" + 0.005*\"says\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"goes\" + 0.005*\"fucking\" + 0.005*\"mean\" + 0.005*\"going\" + 0.005*\"cause\"'),\n",
       " (1,\n",
       "  '0.011*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"want\" + 0.006*\"day\" + 0.005*\"thing\" + 0.005*\"went\" + 0.005*\"going\" + 0.005*\"say\" + 0.005*\"love\"'),\n",
       " (2,\n",
       "  '0.008*\"dad\" + 0.006*\"going\" + 0.006*\"say\" + 0.005*\"hey\" + 0.005*\"shes\" + 0.005*\"mom\" + 0.005*\"want\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"look\"'),\n",
       " (3,\n",
       "  '0.010*\"shit\" + 0.009*\"fuck\" + 0.008*\"fucking\" + 0.006*\"theyre\" + 0.005*\"man\" + 0.005*\"cause\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"life\" + 0.005*\"didnt\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"joke\" + 0.005*\"mom\" + 0.005*\"parents\" + 0.004*\"hasan\" + 0.004*\"jokes\" + 0.004*\"anthony\" + 0.003*\"nuts\" + 0.003*\"dead\" + 0.003*\"tit\" + 0.003*\"twitter\"'),\n",
       " (1,\n",
       "  '0.005*\"mom\" + 0.005*\"jenny\" + 0.005*\"clinton\" + 0.004*\"friend\" + 0.004*\"parents\" + 0.003*\"husband\" + 0.003*\"cow\" + 0.003*\"ok\" + 0.003*\"wife\" + 0.003*\"john\"'),\n",
       " (2,\n",
       "  '0.005*\"bo\" + 0.005*\"gun\" + 0.005*\"guns\" + 0.005*\"repeat\" + 0.004*\"um\" + 0.004*\"ass\" + 0.004*\"eye\" + 0.004*\"contact\" + 0.003*\"son\" + 0.003*\"class\"'),\n",
       " (3,\n",
       "  '0.006*\"ahah\" + 0.004*\"nigga\" + 0.004*\"gay\" + 0.003*\"dick\" + 0.003*\"door\" + 0.003*\"young\" + 0.003*\"motherfucker\" + 0.003*\"stupid\" + 0.003*\"bitch\" + 0.003*\"mad\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: mom, parents\n",
    "* Topic 1: husband, wife\n",
    "* Topic 2: guns\n",
    "* Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'ali'),\n",
       " (0, 'anthony'),\n",
       " (2, 'bill'),\n",
       " (2, 'bo'),\n",
       " (3, 'dave'),\n",
       " (0, 'hasan'),\n",
       " (2, 'jim'),\n",
       " (3, 'joe'),\n",
       " (1, 'john'),\n",
       " (0, 'louis'),\n",
       " (1, 'mike'),\n",
       " (0, 'ricky')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
